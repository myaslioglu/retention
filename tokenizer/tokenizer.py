from .sentencepiece import SentencePieceTokenizer
from .word import WordTokenizer
from pathlib import Path


def get_tokenizer(
    tokenizer_kind: str,
    model_path: Path,
    vocab_size: int,
    algorithm: str = "bpe",
    sample_size: int = 200_000,
    **kwargs,
):
    """Return a tokenizer based on config; builds vocab for the custom tokenizer."""
    if tokenizer_kind == "sentencepiece":
        tk = SentencePieceTokenizer(
            model_path=model_path,
            vocab_size=vocab_size,
            algorithm=algorithm,
            sample_size=sample_size,
            add_special_tokens=True,
            **kwargs,
        )
    elif tokenizer_kind == "custom":
        tk = WordTokenizer(vocab_size=vocab_size)
    else:
        raise NotImplementedError(
            f"ðŸš«Tokenizer of kind: {tokenizer_kind} is not supported! ðŸš«"
        )
    return tk
