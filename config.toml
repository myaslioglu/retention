[model]
    hidden_size     = 512
    max_seq_len     = 512
    vocab_size      = 37000
    dropout_pe      = 0.1
    n_heads         = 8
    #d_k             = 64                 # We can keep it other than d_model / n_heads if needed
    ff_hidden_size  = 2048
    n_layers        = 6
    initializer     = "xavier_uniform"    # xavier_uniform | xavier_normal | kaiming_uniform | kaiming_normal
    attention_kind = "retention"   # "mha" or "retention"

[tokenizer]
    kind            = 'sentencepiece'                 # sentencepiece | custom
    model           = './models/en-de/wmt/shared'     # only applicable for sentencepiece tokenizer
    sample_size     = 200000                          # How many examples to consider to train tokenizer 
    algorithm       = 'bpe'                           # bpe | unigram
    recreate        = false                           # Use the previously created tokenizer model?

[training]
    batch_size     = 16                         # Reduced from 32 to save memory
    epochs         = 10
    learning_rate  = 0.0005
    device         = "auto"                     # auto | cuda | cpu

[dataset]
    path           = "./data/"

[loss]
    type            = "cross_entropy"
    label_smoothing = 0.1

[experiment]
    # If there is a project with the same name but different capitalizations, it will not create a new project
    name            = "Transformer"
    active          = true
    backend         = "wandb"                  # Currently only wandb is supported
    tracking        = "cloud"                  # cloud | offline
    description     = "An experiment to implement \"Attention is all you need\" paper architecture"
