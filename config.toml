[model]
    hidden_size     = 512
    max_seq_len     = 1024
    vocab_size      = 30000
    dropout_pe      = 0.1
    n_heads         = 8                   # Number of heads in a multihead attention block
    #d_k             = 64                 # Size of key vectors in the self-attention layer
    ff_hidden_size  = 1024                # Reduced from 2048 to save memory
    n_layers        = 4                   # Reduced from 6 to save memory

[tokenizer]
    kind            = 'sentencepiece'                 # sentencepiece | custom
    model           = './models/en-de/wmt/shared'     # only applicable for sentencepiece tokenizer
    sample_size     = 200000                          # How many examples to consider to train tokenizer 
    algorithm       = 'bpe'                           # bpe | unigram
    recreate        = false                           # Use the previously created tokenizer model?

[training]
    batch_size     = 16                         # Reduced from 32 to save memory
    epochs         = 10
    learning_rate  = 0.0005
    device         = "auto"                     # auto | cuda | cpu

[dataset]
    path           = "./data/"

[loss]
    type            = "cross_entropy"
    label_smoothing = 0.1

[experiment]
    # If there is a project with the same name but different capitalizations, it will not create a new project
    name            = "Transformer"
    active          = true
    backend         = "wandb"                  # Currently only wandb is supported
    tracking        = "cloud"                  # cloud | offline
    description     = "An experiment to implement \"Attention is all you need\" paper architecture"
