[model]
hidden_size     = 512
max_seq_len     = 1024
vocab_size      = 30000
dropout_pe      = 0.1

# Encoder specific configs
n_heads         = 8          # Number of heads in a multihead attention block
#d_k             = 64         # Size of key vectors in the self-attention layer
ff_hidden_size  = 2048       # Number of neurons in the feed forward network's hidden layer
n_layers        = 6          # Number of layers in the encoder stack

[tokenizer]
kind        = 'sentencepiece'                         # sentencepiece | custom
model       = './models/en-de/wmt/shared'             # only applicable for sentencepiece tokenizer
sample_size = 200000                                  # Number of input pairs to learn from while creating token rules
algorithm   = 'bpe'                                   # bpe | unigram
recreate    = false                                    # Use the previously created tokenizer model?

[training]
batch_size     = 32
epochs         = 10
learning_rate  = 0.0005
device         = "auto"                                    # auto | cuda | mps | cpu

[dataset]
path           = "./data/"

[loss]
type           = "cross_entropy"
label_smoothing = 0.1