# Transformer: Attention Is All You Need

[![Pylint](https://github.com/MayukhSobo/Transformer/actions/workflows/pylint.yml/badge.svg?branch=main&event=push)](https://github.com/MayukhSobo/Transformer/actions/workflows/pylint.yml)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/release/python-390/)

Educational implementation of the Transformer architecture from the ["Attention Is All You Need"](https://arxiv.org/pdf/1706.03762) paper, built with PyTorch.

## ğŸš€ Features

- **Complete Encoder-Decoder Architecture** with cross-attention
- **Modular Design** - each component can be studied independently  
- **Multiple Tokenizers** - SentencePiece and word-level tokenization
- **WMT14 Dataset Integration** - German-English translation
- **Educational Focus** - well-documented code with comprehensive docstrings
- **Production Ready** - proper error handling, logging, and testing

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/MayukhSobo/Transformer.git
cd Transformer

# Using uv (recommended)
uv sync

# Or using pip
pip install -r requirements.txt
```

## ğŸ“– Usage

### Basic Model Creation

```python
from model import build_transformer
from config import Config

config = Config(config_file="config.toml")
transformer, dataset = build_transformer(config)

# Forward pass
output = transformer.forward(src_batch, tgt_batch, src_pad_mask, tgt_pad_mask)
```

### Training

```python
python main.py                    # Train with default config
python main.py --config custom.toml  # Train with custom config
```

### Testing

```python
python test_runner.py             # Run all tests
python test_runner.py pytest     # Run with pytest
python test_runner.py coverage   # Generate coverage report
```

## ğŸ“ Project Structure

```
Transformer/
â”œâ”€â”€ arch/                    # Core transformer modules
â”‚   â”œâ”€â”€ attentions/         # Self, multi-head, and cross-attention
â”‚   â”œâ”€â”€ encoder/            # Encoder components
â”‚   â”œâ”€â”€ decoder/            # Decoder components  
â”‚   â”œâ”€â”€ embedding.py        # Token embeddings
â”‚   â”œâ”€â”€ positional_encoding.py
â”‚   â”œâ”€â”€ feed_forward.py
â”‚   â””â”€â”€ residual_add_norm.py
â”œâ”€â”€ tokenizer/              # Tokenization utilities
â”œâ”€â”€ tests/                  # Test suite
â”œâ”€â”€ data/                   # Dataset directory
â”œâ”€â”€ config.toml             # Model configuration
â”œâ”€â”€ model.py               # Model creation and orchestration
â”œâ”€â”€ train.py               # Training implementation
â”œâ”€â”€ dataset.py             # Dataset loading and preprocessing
â””â”€â”€ main.py                # CLI entry point
```

## âš™ï¸ Configuration

Default model configuration (~100 million parameters):

```toml
[model]
vocab_size = 37000
hidden_size = 512
max_seq_len = 512
n_heads = 8
n_layers = 6
ff_hidden_size = 2048
dropout_pe = 0.1

[tokenizer]
kind = "sentencepiece"    # or "word"
algorithm = "bpe"         # or "unigram"
vocab_size = 32000

[training]
batch_size = 32
epochs = 10
learning_rate = 0.0005

[dataset]
path = "./data"
```

## ğŸ¯ Architecture Highlights

- **Multi-Head Attention**: 8 heads with 64 dimensions each
- **Positional Encoding**: Sinusoidal encoding with learnable parameters
- **Feed-Forward**: Two-layer MLP (512 â†’ 2048 â†’ 512)
- **Residual Connections**: Post-norm architecture with LayerNorm
- **Cross-Attention**: Full encoder-decoder interaction

## ğŸ“Š Current Status

- âœ… **Complete Architecture**: Encoder, decoder, and cross-attention implemented
- âœ… **Tokenization**: SentencePiece and word-level tokenizers
- âœ… **Dataset Integration**: WMT14 German-English with streaming support
- âœ… **Training Pipeline**: Forward pass, loss computation, and optimization
- âœ… **Testing**: Comprehensive test suite with 10.00/10 pylint score

## ğŸ”§ Development

```bash
# Run tests
python test_runner.py

# Run with coverage
python test_runner.py coverage

# Check code quality
pylint $(git ls-files '*.py')

# Format code
black .
```

## ğŸ“š References

- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) - Original paper
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual explanation
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) - Implementation guide

## ğŸ“„ License

MIT License - Free to use for educational purposes.

---

**Educational transformer implementation with complete encoder-decoder architecture and cross-attention, ready for sequence-to-sequence tasks.**
